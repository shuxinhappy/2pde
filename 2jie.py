import torch
from MYLAYER_01 import Net  # 自定义层
import torch
import torch.nn as nn
from torch.autograd import Variable
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# In[2]:
import numpy as np
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#先定义一个模型
class MyNet(torch.nn.Module):
    def __init__(self):
        super(MyNet, self).__init__()  # 第一句话，调用父类的构造函数
        self.mylayer1 = Net()

    def forward(self, x,t):
        out = self.mylayer1(x,t)
        return out
# class Net(nn.Module):
#     def __init__(self):
#         super(Net, self).__init__()
#         self.hidden_layer1 = nn.Linear(2,5)
#         self.hidden_layer2 = nn.Linear(5,5)
#         self.hidden_layer3 = nn.Linear(5,5)
#         self.hidden_layer4 = nn.Linear(5,5)
#         self.hidden_layer5 = nn.Linear(5,5)
#         self.output_layer = nn.Linear(5,1)
#
#     def forward(self, x,t):
#         inputs = torch.cat([x,t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns
#         layer1_out = torch.sigmoid(self.hidden_layer1(inputs))
#         layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))
#         layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))
#         layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))
#         layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))
#         output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer
#         return output

net = Net()
net = net.to(device)
mse_cost_function = torch.nn.MSELoss() # Mean squared error
optimizer = torch.optim.Adam(net.parameters())

def f(x,t, net):
    u = net(x,t) # the dependent variable u is given by the network based on independent variables x,t
    ## Based on our f = du/dx - 2du/dt - u, we need du/dx and du/dt
    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]
    u_2x_2 =torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]
    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]
    pde = u_t - 3*u_2x_2

    return pde

x_bc = np.random.uniform(low=0.0, high=2.0, size=(500,1))
t_bc = np.zeros((500,1))
# compute u based on BC
u_bc1 = 3*np.sin(x_bc-0.5)

# BC tells us that for any x in range[0,2] and time=0, the value of u is given by 6e^(-3x)
# Take say 500 random numbers of x


# In[ ]:


### (3) Training / Fitting
iterations = 1000
previous_validation_loss = 99999999.0
for epoch in range(iterations):
    optimizer.zero_grad()  # to make the gradients zero

    # Loss based on boundary conditions
    pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)
    pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)
    pt_u_bc = Variable(torch.from_numpy(u_bc1).float(), requires_grad=False).to(device)

    net_bc_out = net(pt_x_bc, pt_t_bc)  # output of u(x,t)
    mse_u = mse_cost_function(net_bc_out, pt_u_bc)

    # Loss based on PDE
    x_collocation = np.random.uniform(low=0.0, high=2.0, size=(500, 1))
    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(500, 1))
    all_zeros = np.zeros((500, 1))

    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)
    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)
    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)

    f_out = f(pt_x_collocation, pt_t_collocation, net)  # output of f(x,t)
    mse_f = mse_cost_function(f_out, pt_all_zeros)

    # Combining the loss functions
    loss = mse_u + mse_f

    loss.backward()  # This is for computing gradients using backward propagation
    optimizer.step()  # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta

    with torch.autograd.no_grad():
        print(epoch, "Traning Loss:", loss.data)
# In[17]:

import matplotlib
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import numpy as np

fig = plt.figure()
ax = fig.gca(projection='3d')

x=np.arange(0,2,0.02)
t=np.arange(0,1,0.02)
ms_x, ms_t = np.meshgrid(x, t)
## Just because meshgrid is used, we need to do the following adjustment
x = np.ravel(ms_x).reshape(-1,1)
t = np.ravel(ms_t).reshape(-1,1)

pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)
pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device)
pt_u = net(pt_x,pt_t)
u=pt_u.data.cpu().numpy()
ms_u = u.reshape(ms_x.shape)

surf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)



ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))

fig.colorbar(surf, shrink=0.5, aspect=5)

plt.show()
#
#
# # In[20]:
#
#
# # Save Model
# torch.save(net.state_dict(), "model_uxt.pt")
